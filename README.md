# Deep-Learning
An activation function in neural networks introduces non-linearity, enabling complex relationships between inputs and outputs. Popular functions like sigmoid, ReLU, and tanh play a crucial role in efficient learning and preventing vanishing or exploding gradients during training.
